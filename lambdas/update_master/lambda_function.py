"""
Lambda function to update cleaned/master/master.csv when new cleaned files are generated by parse_statement lambda function.
This function is triggered by S3 events when new cleaned files are uploaded.
"""

import boto3
import pandas as pd
import logging

from botocore.exceptions import ClientError
from urllib.parse import unquote_plus

logger = logging.getLogger()
logger.setLevel(logging.INFO)

s3 = boto3.client('s3')

BUCKET = 'aws-budget-buddy'
MASTER_KEY = 'cleaned/master/master.csv'

def lambda_handler(event, context):
    logger.info("Lambda triggered with event:")
    logger.info(event)

    for record in event['Records']:
        bucket = record['s3']['bucket']['name']
        key = unquote_plus(record['s3']['object']['key'])

        # if master.csv is updated, it will trigger this lambda again
        # this prevents recursive self-triggering
        if key == MASTER_KEY:
            logger.info("Skipping master.csv to avoid self-trigger.")
            continue

        logger.info(f"Processing file from bucket: {bucket}, key: {key}")

        try:
            obj = s3.get_object(Bucket=bucket, Key=key)
            new_data = pd.read_csv(obj['Body'])
            logger.info(f"Read {len(new_data)} rows from new CSV")

            # read master.csv
            try:
                master_obj = s3.get_object(Bucket=BUCKET, Key=MASTER_KEY)
                master_df = pd.read_csv(master_obj['Body'])
                logger.info(f"Read {len(master_df)} rows from master.csv")

            except ClientError as e:
                if e.response['Error']['Code'] == 'NoSuchKey':
                    logger.warning("master.csv not found. Starting fresh.")
                    master_df = pd.DataFrame(columns=new_data.columns)
                else:
                    raise

            # combine and drop exact duplicates
            combined = pd.concat([master_df, new_data], ignore_index=True)
            before = len(combined)
            combined = combined.drop_duplicates()
            after = len(combined)
            logger.info(f"Dropped {before - after} duplicate rows. Final row count: {after}")

            # update master
            s3.put_object(
                Bucket=BUCKET, 
                Key=MASTER_KEY, 
                Body=combined.to_csv(index=False).encode('utf-8')
            )

            logger.info("Successfully updated master.csv")

        except Exception as e:
            logger.exception(f"Failed to process file {key}: {e}")
            raise
