"""
Lambda function to update master expenses file when new cleaned files are generated by parse_statement lambda function.
This function is triggered by S3 events when new cleaned files are uploaded.
"""

import boto3
import pandas as pd
import logging

from botocore.exceptions import ClientError
from urllib.parse import unquote_plus

logger = logging.getLogger()
logger.setLevel(logging.INFO)

s3 = boto3.client('s3')

BUCKET = 'aws-budget-buddy'
MASTER_KEY = 'categorized_expenses.csv'

def lambda_handler(event, context):
    logger.info("Lambda triggered with event:")
    logger.info(event)

    for record in event['Records']:
        bucket = record['s3']['bucket']['name']
        key = unquote_plus(record['s3']['object']['key'])

        logger.info(f"Processing file from bucket: {bucket}, key: {key}")

        try:
            obj = s3.get_object(Bucket=bucket, Key=key)
            new_data = pd.read_csv(obj['Body'])
            logger.info(f"Read {len(new_data)} rows from new CSV")

            # read master file
            # if master does not exist, start with an empty DataFrame
            try:
                master_obj = s3.get_object(Bucket=BUCKET, Key=MASTER_KEY)
                master_df = pd.read_csv(master_obj['Body'])
                logger.info(f"Read {len(master_df)} rows from master file")

            except ClientError as e:
                if e.response['Error']['Code'] == 'NoSuchKey':
                    logger.warning("master file not found. Starting fresh.")
                    master_df = pd.DataFrame(columns=new_data.columns)
                else:
                    raise

            # combine and drop exact duplicates
            combined = pd.concat([master_df, new_data], ignore_index=True)
            before = len(combined)
            combined = combined.drop_duplicates()
            after = len(combined)
            logger.info(f"Dropped {before - after} duplicate rows. Final row count: {after}")

            # update master
            s3.put_object(
                Bucket=BUCKET, 
                Key=MASTER_KEY, 
                Body=combined.to_csv(index=False).encode('utf-8')
            )

            logger.info(f"Successfully updated master file at {MASTER_KEY}")

        except Exception as e:
            logger.exception(f"Failed to process file {key}: {e}")
            raise
