"""
Lambda function to update master expenses file when new cleaned files are generated by parse_statement lambda function.
This function is triggered by S3 events when new cleaned files are uploaded.
"""

import boto3
import pandas as pd
import logging

from io import BytesIO
from urllib.parse import unquote_plus
from botocore.exceptions import ClientError

logger = logging.getLogger()
logger.setLevel(logging.INFO)

s3 = boto3.client('s3')

BUCKET = 'aws-budget-buddy'
MASTER_KEY = 'categorized_expenses.parquet'

def lambda_handler(event, context):
    logger.info("Lambda triggered with event:")
    logger.info(event)

    for record in event['Records']:
        bucket = record['s3']['bucket']['name']
        key = unquote_plus(record['s3']['object']['key'])

        logger.info(f"Processing file from bucket: {bucket}, key: {key}")

        try:
            obj = s3.get_object(Bucket=bucket, Key=key)
            new_data = pd.read_csv(
                obj['Body'],
                dtype={
                    'description': 'str',
                    'amount': 'float64',
                    'statement_issuer': 'str',
                    'category': 'str',
                    'notes': 'str'
                },
                parse_dates=['transaction_date'],
                infer_datetime_format=True
            )

            logger.info(f"Read {len(new_data)} rows from new CSV")

            # Load master file (if it exists)
            try:
                master_obj = s3.get_object(Bucket=BUCKET, Key=MASTER_KEY)
                master_body = BytesIO(master_obj['Body'].read())
                master_df = pd.read_parquet(master_body)
                logger.info(f"Read {len(master_df)} rows from master file")

            except ClientError as e:
                if e.response['Error']['Code'] == 'NoSuchKey':
                    logger.warning("Master file not found. Starting fresh.")
                    master_df = pd.DataFrame(columns=new_data.columns)
                else:
                    raise

            # include required columns in master schema
            # handles cases where master is empty or missing 
            for col in ['category', 'notes']:
                if col not in master_df.columns:
                    master_df[col] = pd.NA

            # Merge and deduplicate
            # we ignore columns that are not in both dataframes, handling categorized expenses gracefully
            combined = pd.concat([master_df, new_data], ignore_index=True)
            deduplication_cols = [col for col in new_data.columns if col in combined.columns]
            before = len(combined)
            combined = combined.drop_duplicates(subset=deduplication_cols)
            after = len(combined)
            logger.info(f"Dropped {before - after} duplicate rows. Final row count: {after}")

            # Save to Parquet in memory
            out_buffer = BytesIO()
            combined.to_parquet(out_buffer, index=False, compression='snappy')

            # Upload updated master file
            s3.put_object(
                Bucket=BUCKET,
                Key=MASTER_KEY,
                Body=out_buffer.getvalue()
            )

            logger.info(f"Successfully updated master file at {MASTER_KEY}")

        except Exception as e:
            logger.exception(f"Failed to process file {key}: {e}")
            raise
