"""
Lambda function to update cleaned/master/master.csv when new cleaned files are generated by parse_statement lambda function.
This function is triggered by S3 events when new cleaned files are uploaded.
"""

import boto3
import pandas as pd
import logging
from botocore.exceptions import ClientError

logger = logging.getLogger()
logger.setLevel(logging.INFO)

s3 = boto3.client('s3')

BUCKET = 'aws-budget-buddy'
MASTER_KEY = 'cleaned/master/master.csv'

def lambda_handler(event, context):
    logger.info("Lambda triggered with event:")
    logger.info(event)

    for record in event['Records']:
        bucket = record['s3']['bucket']['name']
        key = record['s3']['object']['key']

        # the S3 trigger rules for this lambda will explicitly exclude master.csv
        # so this check is just a safety measure
        if key == MASTER_KEY:
            logger.info("Skipping master.csv to avoid self-trigger.")
            continue

        logger.info(f"Processing file from bucket: {bucket}, key: {key}")

        try:
            obj = s3.get_object(Bucket=bucket, Key=key)
            new_data = pd.read_csv(obj['Body'])
            logger.info(f"Read {len(new_data)} rows from new CSV")

            # read master.csv
            try:
                master_obj = s3.get_object(Bucket=BUCKET, Key=MASTER_KEY)
                master_df = pd.read_csv(master_obj['Body'])
                logger.info(f"Read {len(master_df)} rows from master.csv")

            except ClientError as e:
                if e.response['Error']['Code'] == 'NoSuchKey':
                    logger.warning("master.csv not found. Starting fresh.")
                    master_df = pd.DataFrame(columns=new_data.columns)
                else:
                    raise

            # combine and drop exact duplicates
            combined = pd.concat([master_df, new_data], ignore_index=True)
            before = len(combined)
            combined = combined.drop_duplicates()
            after = len(combined)
            logger.info(f"Dropped {before - after} duplicate rows. Final row count: {after}")

            # update master
            s3.put_object(
                Bucket=BUCKET, 
                Key=MASTER_KEY, 
                Body=combined.to_csv(index=False).encode('utf-8')
            )

            logger.info("Successfully updated master.csv")

        except Exception as e:
            logger.exception(f"Failed to process file {key}: {e}")
            raise
